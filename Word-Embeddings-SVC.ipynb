{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:40.556862Z",
     "start_time": "2018-11-26T13:16:33.588237Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-24T22:12:28.166202Z",
     "start_time": "2018-11-24T22:12:28.159058Z"
    }
   },
   "source": [
    "### 1: Loading the data set from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:44.161164Z",
     "start_time": "2018-11-26T13:16:42.206160Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "train_data = pd.read_csv('./data/all/train.csv')\n",
    "test_data = pd.read_csv('./data/all/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:17:52.795307Z",
     "start_time": "2018-11-26T13:17:52.788295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:44.234949Z",
     "start_time": "2018-11-26T13:16:44.231253Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes/functions to extract/prepare the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:44.304213Z",
     "start_time": "2018-11-26T13:16:44.293723Z"
    }
   },
   "outputs": [],
   "source": [
    "# PrepareData\n",
    "class PrepareData:\n",
    "    \"\"\"\n",
    "    Class to perpare the data for taraining, and testing \n",
    "    The class provide instance and static methods to prepare the data.\n",
    "\n",
    "    Args:\n",
    "    data_set : data set\n",
    "    \"\"\"\n",
    "\n",
    "    # get and initialize the dataset\n",
    "    def __init__(self, data_set):\n",
    "        self.data_set = data_set\n",
    "\n",
    "    def split_data(self, train_size=0.7, test_size=0.30, random_state=42, shuffle=True):\n",
    "        \"\"\" This method split the data into train data set and test data set\n",
    "        by default it splits the data as train_data = 70% and test_data = 30%\n",
    "\n",
    "        Args:\n",
    "        train_size : percentage of train data, default value - 0.7\n",
    "                        default:0.7\n",
    "        test_size : percetage of test data, default value - 0.3\n",
    "                        default:0.3\n",
    "        Returns:\n",
    "        train_x : train data\n",
    "        test_x : test data\n",
    "        train_y :train data\n",
    "        test_y : test data\n",
    "        \"\"\"\n",
    "        train_x, test_x = train_test_split(\n",
    "            self.data_set, random_state=random_state, test_size=test_size, shuffle=True)\n",
    "\n",
    "        return train_x, test_x\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_data_func(train, test):\n",
    "        \"\"\" This method split the data into train data set and test data set\n",
    "        by default it splits the data as train_data = 70% and test_data = 30%\n",
    "\n",
    "        Args:\n",
    "        train_percent : percentage of train data, default value - 0.7\n",
    "                        default:0.7\n",
    "        test_pecent : percetage of test data, default value - 0.3\n",
    "                        default:0.3\n",
    "        seed : None optional\n",
    "\n",
    "        Returns:\n",
    "        train_data : tranin data\n",
    "        test_data : test data\n",
    "        \"\"\"\n",
    "        train_data = train[:20000]\n",
    "        test_data = test[:7000]\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_test_data(labels, data_set):\n",
    "        \"\"\"\n",
    "        Utility method to prepare the test data. \n",
    "        This method keeps the lables which are required for testing \n",
    "        by removing other lables from data frame\n",
    "\n",
    "        Args:\n",
    "        labels : list of expected lables in test data set\n",
    "        data_set : test data set\n",
    "\n",
    "        Returns:\n",
    "        data_set : processed dataset\n",
    "        \"\"\"\n",
    "\n",
    "        deleting_labels = [label for label in list(\n",
    "            data_set.columns) if label not in labels]\n",
    "        data_set = data_set.drop(deleting_labels, axis=1)\n",
    "        return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:44.764245Z",
     "start_time": "2018-11-26T13:16:44.758856Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = PrepareData.prepare_data_func(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:45.119855Z",
     "start_time": "2018-11-26T13:16:45.094024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:45.620327Z",
     "start_time": "2018-11-26T13:16:45.574349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.097350</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.051050</td>\n",
       "      <td>0.009050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.296441</td>\n",
       "      <td>0.104773</td>\n",
       "      <td>0.225727</td>\n",
       "      <td>0.057784</td>\n",
       "      <td>0.220105</td>\n",
       "      <td>0.094702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic  severe_toxic       obscene        threat        insult  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       0.097350      0.011100      0.053850      0.003350      0.051050   \n",
       "std        0.296441      0.104773      0.225727      0.057784      0.220105   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       identity_hate  \n",
       "count   20000.000000  \n",
       "mean        0.009050  \n",
       "std         0.094702  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:46.152643Z",
     "start_time": "2018-11-26T13:16:46.137406Z"
    }
   },
   "outputs": [],
   "source": [
    "class CleanData:\n",
    "    \"\"\"\n",
    "    Utiliy class to clean the data. provides instance and static methods \n",
    "    for cleaning the train/test data\n",
    "\n",
    "    Args:\n",
    "    data_set : data set to be cleaned. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_set):\n",
    "        self.data_set = data_set\n",
    "\n",
    "    def fill_null(self):\n",
    "        \"\"\"\n",
    "        Method to fill all the null values in the data set\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        data_set : data set with non null values\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_set.fillna(' ')\n",
    "        return self.data_set\n",
    "\n",
    "    def clean_text(self, field):\n",
    "        \"\"\"\n",
    "        Method to clean the text data. \n",
    "\n",
    "        Args:\n",
    "        field : class label in the data set\n",
    "\n",
    "        Returns:\n",
    "        data_set : cleaned data set\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"http\\S+\", \"\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"http\", \"\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"@\\S+\", \"\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"@\", \"at\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"what's\", \"what is\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"\\'ve\", \" have\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"can't\", \"can not\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"don't\", \"do not\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"weren't\", \"were not\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"doesn't\", \"does not\")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"i'm\", \"i am \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"\\'re\", \" are \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(\n",
    "            r\"\\'d\", \" would \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"\\'s\", \" \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"\\W\", \" \")\n",
    "        self.data_set[field] = self.data_set[field].str.replace(r\"\\s+\", \" \")\n",
    "        self.data_set[field] = self.data_set[field].str.lower()\n",
    "        return self.data_set\n",
    "\n",
    "    def remove_stop_words(self, field):\n",
    "        \"\"\"\n",
    "        Method to remove the stopwords from the text.\n",
    "\n",
    "        Args:\n",
    "        field: class label in the dataset\n",
    "\n",
    "        Returns:\n",
    "        data_set : data set without stopwords.\n",
    "        \"\"\"\n",
    "        self.data_set[field].apply(lambda x: ' '.join(\n",
    "            [word for word in x.split() if word not in stopwords.words('english')]))\n",
    "        return self.data_set\n",
    "\n",
    "    def get_text(self, field):\n",
    "        \"\"\"\n",
    "        Utility method to get particular column from data set\n",
    "\n",
    "        Args:\n",
    "        field : name of the required field\n",
    "\n",
    "        Returns:\n",
    "        data_set : returns the text of metioned field\n",
    "        \"\"\"\n",
    "\n",
    "        return self.data_set[field]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_text(data_set_one, data_set_two, field):\n",
    "        \"\"\"\n",
    "        Static method to combine a particular column of two data sets.\n",
    "\n",
    "        Args:\n",
    "        data_set_one : data frame \n",
    "        data_set_two : data frame\n",
    "        field : column name \n",
    "\n",
    "        Returns:\n",
    "        data_set : new data set\n",
    "\n",
    "        \"\"\"\n",
    "        data_set = pd.concat([data_set_one[field], data_set_two[field]])\n",
    "        return data_set\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_text(data_set_one, data_set_two):\n",
    "        \"\"\"\n",
    "        Static method to combine two data sets.\n",
    "\n",
    "        Args:\n",
    "        data_set_one : first data set of type data frame\n",
    "        data_set_two : second data set of type \n",
    "\n",
    "        Returns:\n",
    "        data_set_merged : new combined data set\n",
    "        \"\"\"\n",
    "        data_set_merged = pd.concat([data_set_one, data_set_two])\n",
    "        return data_set_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class provides some utility methods to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T00:06:18.452800Z",
     "start_time": "2018-11-25T00:04:40.974Z"
    }
   },
   "source": [
    "Inorder to proceed further with generating the word embeddings, we need to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:46.722228Z",
     "start_time": "2018-11-26T13:16:46.697297Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanng the training data.\n",
    "clean_train_data = CleanData(train_data)\n",
    "train_data = clean_train_data.fill_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:18:54.461779Z",
     "start_time": "2018-11-26T13:18:54.453483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:46.889147Z",
     "start_time": "2018-11-26T13:16:46.876726Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanng the training data.\n",
    "clean_train_data = CleanData(test_data)\n",
    "test_data = clean_train_data.fill_null()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:19:40.272163Z",
     "start_time": "2018-11-26T13:19:40.265645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Implementation, Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:47.260916Z",
     "start_time": "2018-11-26T13:16:47.253323Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    \"\"\"\n",
    "    This class provides methods to generate the word embeddings.\n",
    "\n",
    "    Args:\n",
    "    train_data : training data set\n",
    "    test_data : test data set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def get_embeddings(self, field):\n",
    "        \"\"\"\n",
    "        This method generates the word wmbeddings for train and test data.\n",
    "        It makes use of Word2Vec and train the embeddings.\n",
    "\n",
    "        Args:\n",
    "        field : class name in the data set\n",
    "\n",
    "        Returns\n",
    "        word_embedding : WordVector\n",
    "        \"\"\"\n",
    "        df = self.train_data[field].append(self.test_data[field])\n",
    "        vocab = df.apply(lambda x: str(x).strip().split())\n",
    "        print(len(vocab))\n",
    "        models = Word2Vec(vocab, size=100)\n",
    "        word_embedding = dict(zip(models.wv.index2word, models.wv.vectors))\n",
    "        return word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below class SentEmbeddingVectorizer implements the fit and transform methods to transform the word embeddings to sentence level embeddings to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:47.708867Z",
     "start_time": "2018-11-26T13:16:47.701136Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentEmbeddingVectorizer:\n",
    "    \"\"\"\n",
    "    This class provides the fit and transform methods to move from word embedding to sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "    word_embedding : Word2Vec \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding):\n",
    "        self.word_embedding = word_embedding\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model according to the given training data.\n",
    "\n",
    "        Args:\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target vector relative to X.\n",
    "        sample_weight : array-like, shape (n_samples,) optional\n",
    "                        Array of weights that are assigned to individual samples. \n",
    "                        If not provided, then each sample is given unit weight.\n",
    "\n",
    "        Returns:\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Method to transform the word embeddings to sentence embedding\n",
    "\n",
    "        Args:\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        \"\"\"\n",
    "        return np.array([np.mean([self.word_embedding[word] for word in words if word in self.word_embedding] or [np.zeros(self.dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below class TfidfSentEmbeddingVectorizer implements the fit and transform methods to transform the word embeddings to sentence level embeddings to train the model, this class also adds Tfidf weightings which supposed to give better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:48.355021Z",
     "start_time": "2018-11-26T13:16:48.346583Z"
    }
   },
   "outputs": [],
   "source": [
    "class TfidfSentEmbeddingVectorizer:\n",
    "    \"\"\"\n",
    "    This class provides the fit and transform methods to move from word embedding to sentence embeddings.\n",
    "    It adds Tfidf weights.\n",
    "\n",
    "    Args:\n",
    "    word_embedding : Word2Vec\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_embedding):\n",
    "        self.word_embedding = word_embedding\n",
    "        self.word_weight = None\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model according to the given training data.\n",
    "\n",
    "        Args:\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target vector relative to X.\n",
    "        sample_weight : array-like, shape (n_samples,) optional\n",
    "                        Array of weights that are assigned to individual samples. \n",
    "                        If not provided, then each sample is given unit weight.\n",
    "\n",
    "        Returns:\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        tfidfs = TfidfVectorizer(ngram_range=(1, 1), analyzer=lambda x: x)\n",
    "        tfidfs.fit(X)\n",
    "        max_idf = max(tfidfs.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, [(w, tfidfs.idf_[i]) for w, i in tfidfs.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Method to transform the word embeddings to sentence embedding\n",
    "\n",
    "        Args:\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.array([np.mean([self.word2vec[w] * self.word2weight[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the classifier Logistic Regression with Naive Bayes features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:16:50.085546Z",
     "start_time": "2018-11-26T13:16:50.069135Z"
    }
   },
   "outputs": [],
   "source": [
    "class NbLogRegClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    \"\"\"\n",
    "    The Logistic Regression classifier makes use of basic Naive Bayes feature.\n",
    "    The classifier inherits from BaseEstimator and ClassifierMixin sklearn's base package.\n",
    "\n",
    "    Args:\n",
    "    C :  Inverse of regularization strength; must be a positive float. \n",
    "        Like in support vector machines, smaller values specify stronger regularization.\n",
    "        default: 1.0\n",
    "    solver : Algorithm to use in the optimization problem.\n",
    "            default: 'sag'\n",
    "            prefered for multiclass problem - 'newton-cg’, ‘sag’, ‘saga’ or ‘lbfgs'\n",
    "    n_jobs : Number of CPU cores used when parallelizing over classes if multi_class=’ovr’”.\n",
    "            default: -1 (using all processors)\n",
    "    max_iter : Maximum number of iterations taken for the solvers to converge.\n",
    "            default: 4000\n",
    "    dual: Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. \n",
    "          Prefer dual=False when n_samples > n_features.\n",
    "          default: False\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=4, dual=False, n_jobs=-1, solver='sag', max_iter=4000):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Args:\n",
    "        X : array_like or sparse matrix, shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        C : array, shape [n_samples]\n",
    "            Predicted class label per sample.\n",
    "\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(X.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Probability estimates.\n",
    "\n",
    "        The returned estimates for all classes are ordered by the\n",
    "        label of classes.\n",
    "\n",
    "        Args:\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns:\n",
    "        T : \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(X.multiply(self._r))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Fit the model according to the given training data.\n",
    "\n",
    "        Args:\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target vector relative to X.\n",
    "        sample_weight : array-like, shape (n_samples,) optional\n",
    "                        Array of weights that are assigned to individual samples. \n",
    "                        If not provided, then each sample is given unit weight.\n",
    "\n",
    "        Returns:\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = y.values\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        def pr(X, y_i, y):\n",
    "            \"\"\"\n",
    "            Method to implement the basic Naive Bayes feature\n",
    "            \"\"\"\n",
    "            p = X[y == y_i].sum(0)\n",
    "            return (p+1) / ((y == y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(X, 1, y) / pr(X, 0, y)))\n",
    "        x_nb = X.multiply(self._r)\n",
    "        self._clf = LogisticRegression(\n",
    "            C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T20:52:48.968739Z",
     "start_time": "2018-11-25T20:52:48.857510Z"
    }
   },
   "source": [
    "Getting the word embeddingd for traiing and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T13:17:04.811400Z",
     "start_time": "2018-11-26T13:16:55.443368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000\n"
     ]
    }
   ],
   "source": [
    "word_embeding = WordEmbedding(train_data=train_data, test_data=test_data)\n",
    "word_vector = word_embeding.get_embeddings(field='comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:33:09.104723Z",
     "start_time": "2018-11-26T04:33:09.098638Z"
    }
   },
   "outputs": [],
   "source": [
    "svc_model=Pipeline([(\"word_vector\",SentEmbeddingVectorizer(word_vector)),(\"multilabel\",OneVsRestClassifier(LinearSVC(random_state=0)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T00:31:28.940185Z",
     "start_time": "2018-11-26T00:31:28.935191Z"
    }
   },
   "source": [
    "### 4. Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:34:15.512698Z",
     "start_time": "2018-11-26T04:34:15.506060Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train=train_data[[i for i in train_data.columns if i not in [\"comment_text\",\"id\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:35:54.460605Z",
     "start_time": "2018-11-26T04:35:21.638969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word_vector', <__main__.SentEmbeddingVectorizer object at 0x1a2c9309e8>), ('multilabel', OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "svc_model.fit(train_data['comment_text'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:50:04.319145Z",
     "start_time": "2018-11-26T04:49:59.078685Z"
    }
   },
   "outputs": [],
   "source": [
    "# predicting\n",
    "pred=svc_model.predict(train_data['comment_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
